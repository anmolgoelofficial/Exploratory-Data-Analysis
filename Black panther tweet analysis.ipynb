{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11561521",
   "metadata": {},
   "source": [
    "### Black Panther Tweets Sentiment Anaysis\n",
    "\n",
    "The Analysis uses the twint library to mine tweets with keywords BlackPanther, WakandaForever, BlackPanther2, and Black Panther. \n",
    "The tweets will be analysed and preprocessed to make it clean(free of emojis, punctuations etc). The Vader Sentiment library will be used for the sentiment analysis and the analysis are visualized and communicated better in power BI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945f0eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing necessary libraries\n",
    "import twint\n",
    "import nest_asyncio\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "import preprocessor as p\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d12ba5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nest_asyncio.apply()\n",
    "c = twint.Config()\n",
    "c.Search = \"BlackPanther OR WakandaForever OR BlackPanther2 OR Black Panther\" # topic\n",
    "c.Limit = 2000000 # number of Tweets to scrape\n",
    "c.Lang= \"en\"\n",
    "c.Store_csv = True # store tweets in a csv file\n",
    "c.Output = \"tweets.csv\" # path to csv file\n",
    "twint.run.Search(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086a7fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('tweets.csv') #reading in tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c584a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info() #columns inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add8586c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping columns I won't be needing\n",
    "df=df.drop(['trans_dest','trans_src','translate',\"retweet_date\",'retweet_id','user_rt','user_rt_id','source','geo','near','quote_url','hashtags'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9eb472a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum() #checking for columns with missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d7556a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True,axis=1) #dropping missing columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa537ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df[['id','date','time','username','tweet','retweets_count','likes_count','retweet']] #selecting only necessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acec354",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.reset_index() #reseting index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1624d46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop([\"index\"], axis=1) #dropping the old index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2568548c",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \"\"\" \n",
    "    This function removes all hashtags found in tweets\n",
    "    tweet: string\n",
    "    a tweet that consists of hashtags to be cleaned\n",
    "    \n",
    "    \n",
    "    returns \n",
    "    -------\n",
    "    tweet: string\n",
    "    a tweet without hastags\n",
    "    \n",
    "    \"\"\"\n",
    "def hashtag_removal(tweet):\n",
    "    tweet=tweet.lower()\n",
    "    patterns=re.findall(\"#[\\w]*\",tweet)\n",
    "    for i in patterns:\n",
    "        tweet=tweet.replace(i,'')\n",
    "    return tweet\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577b1b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#appying the hashtag_removal function to the tweet column \n",
    "df['clean_tweet']=df['tweet'].apply(hashtag_removal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8796a84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using the tweet preprocessor library to get rid of emojis\n",
    "df['clean_tweet']=df['clean_tweet'].apply(p.clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b4bec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \"\"\" \n",
    "    This function removes all punctuations specified in the function from the tweets\n",
    "    r: string\n",
    "    a tweet that consists of punctuations to be cleaned\n",
    "    \n",
    "    \n",
    "    returns \n",
    "    -------\n",
    "    r: string\n",
    "    a tweet without punctuations\n",
    "    \n",
    "    \"\"\"\n",
    "def punctuation_removal(r):\n",
    "    patterns=re.findall(r'&(\\w+);', r)\n",
    "    for i in patterns:\n",
    "        r=r.replace(\"&{i};\",\"\")\n",
    "    punc = '''!()-[]{};:'\"\"\\,<>./?@#$%^&*_~'''\n",
    "    for ele in r:\n",
    "        if ele in punc:\n",
    "            r = r.replace(ele, \"\")\n",
    "    return r\n",
    "df['clean_tweet']=df['clean_tweet'].apply(punctuation_removal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9deea18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizing and lemmatizing each words \n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "w_tokenizer = TweetTokenizer()\n",
    "def lemmatize_text(text):\n",
    "    return [(lemmatizer.lemmatize(w)) for w in w_tokenizer.tokenize((text))]\n",
    "df['tokenized_tweet'] = df['clean_tweet'].apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f65772",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "df['tokenized_tweet'] = df['tokenized_tweet'].apply(lambda x: [item for item in x if item not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a51d695",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \"\"\" \n",
    "    This function creates a sentiment analysis for each tweet\n",
    "    tweet: string\n",
    "    a tweet that sentiment analysis needs to be performed on\n",
    "    \n",
    "    \n",
    "    returns \n",
    "    -------\n",
    "    positive: string\n",
    "    if the negative score is less than positive\n",
    "    \n",
    "    negative: string\n",
    "    if the negative score is greater than positive\n",
    "    \n",
    "    neutral: string\n",
    "    if both above condition is not met.\n",
    "    \n",
    "    \"\"\"\n",
    "def sentiment_analyzer(tweet):\n",
    "    sentiment= SentimentIntensityAnalyzer()\n",
    "    score= sentiment.polarity_scores(tweet)\n",
    "    if score['neg']<score['pos']:\n",
    "        return \"positive\"\n",
    "    elif score['neg']> score['pos']:\n",
    "        return \"negative\"\n",
    "    else:\n",
    "        return \"neutral\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf7242f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#appying the function to the clean tweet\n",
    "df['Sentiments']=df['clean_tweet'].apply(sentiment_analyzer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d560e9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of casts I want to search for\n",
    "casts=[\"tchalla\",'shuri','nakia','okoye',\"m'Baku\",\"riri\",\"aneka\",\"namor\",\"chadwick\",\"tenoch\",\"letitia\",\"ramonda\",\"angela\",\"mabel\",\n",
    "      \"michaela\",\"danai\",\"lupita\",\"domique\",\"winston\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3289677",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \"\"\" \n",
    "    This function gets the casts names that are both in cast list and the tweet \n",
    "    tweet: string\n",
    "    a tweet\n",
    "    \n",
    "    \n",
    "    returns \n",
    "    -------\n",
    "    []:a list \n",
    "\n",
    "    a list of cast names present in the tweet\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "def getcast(tweet):\n",
    "    BP_cast = [char for char in casts if char in tweet] \n",
    "    return \" \".join(BP_cast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b205e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cast']=df[\"clean_tweet\"].apply(getcast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fee1001",
   "metadata": {},
   "outputs": [],
   "source": [
    "cast_list = df['cast'].tolist()\n",
    "\n",
    "# Iterate over all cast names and split where there is more than one cast\n",
    "cast = []\n",
    "for item in cast_list:\n",
    "    item = item.split()\n",
    "    for i in item:\n",
    "        cast.append(i)\n",
    "\n",
    "# Determine Unique count of all cast\n",
    "counts = Counter(cast)\n",
    "cast_df = pd.DataFrame.from_dict(counts, orient='index').reset_index()\n",
    "cast_df.columns = ['cast', 'Count']\n",
    "cast_df.sort_values(by='Count', ascending=False, inplace=True)\n",
    "cast_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f061df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the new dataframe to a csv file\n",
    "df.to_csv('clean_tweets.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7662dde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
